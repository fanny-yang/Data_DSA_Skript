\section{Analysis}
\authors{Franziska Wilfinger, Jan Fritz, Florentine Klepel}  

Analysis ist ein wichtiges Teilgebiet der Mathematik. Sie ist Grundlage für Optimierung und somit auch von großer Bedeutung für das Maschinelle Lernen. Im Kurs war für uns besonders die Stetigkeit und Differenzierbarkeit von ein- und mehrdimensionalen Funktionen wichtig.

\subsection{Stetigkeit und Differenzierbarkeit}

Wir beginnen mit einigen theoretischen Grundlagen der Analysis.
\begin{Def} Die Zahl $L$ heißt \emph{Grenzwert} der Funktion $f(x)$ für $x$ gegen $a$, wenn es für alle $\epsilon > 0$ ein $\delta > 0$ existiert, sodass für alle $|x-a| < \delta$ gilt $|f(x)-L|<\epsilon$
\end{Def}
\begin{Def}
Eine Funktion $f: D \subseteq \mathbb{R}\rightarrow \mathbb{R}$ ist \emph{stetig}, wenn $\lim_{w\rightarrow x}f(w)=f(x)$ auf ihrem gesamten Definitionsbereich gilt, das heißt sie ist stetig in $x$ für alle $x\in D$.
\end{Def}
\begin{Def}
Eine Funktion $f:\mathbb{R}\rightarrow \mathbb{R}$ ist an einer Stelle $x$ \emph{differenzierbar}, wenn eine lineare Abbildung $ l_ x (h)$ und ein Restglied $r(h)$ existieren, sodass $f(x+h)=f(x)+l_{x}(h)+r(h)$ wobei $\lim_{h\rightarrow 0} r(h)/h= 0$ gilt.
\end{Def}
Wir betrachten eine einfache Folgerung aus diesen Definitionen.


\begin{lemma}
Eine Funktion $f:\mathbb{R} \rightarrow \mathbb{R}$, welche differenzierbar an der Stelle $x \in \mathbb{R}$ ist, ist dort auch stetig.
\end{lemma}

\begin{proof}
Wir überlegen zuerst, welche Bedingungen gegeben sind. Eine Funktion ist an der Stelle $x$ differenzierbar, wenn $f(x+h)=f(x)+l_x(h)+ r(h)$  mit $\lim_{h \rightarrow 0} r(h)/h= 0$. Um die Stetigkeit nachzuweisen müssen wir zeigen, dass $\lim_{w \rightarrow x} f(w)=f(x)$. Mit $w = x+h$ ist dies äquivalent zu $\lim_{h \rightarrow 0} f(x)+l_x(h)+r(h)=f(x)$. Es genügt also zu zeigen, dass  $l_x(h)$ und $r(h)$ gegen Null gehen.
\begin{itemize}
  \item Würde $r(h)$ nicht gegen Null gehen, so würde $r(h)/h$ nicht gegen Null gehen. Widerspruch zur Differenzierbarkeit.
  \item Der lineare Term $l_x(h)$ kann auch als $f'(x) \cdot h$ dargestellt werden. Da $h$ gegen Null geht folgt $l_x(h)\rightarrow 0$.
\end{itemize}
Insgesamt folgt $\lim_{w\rightarrow x} f(w) = f(x)$.
\end{proof}

\subsection{Gradient}
\begin{Def}
Der \emph{Gradient} $\nabla f(x)$ einer Funktion $f:\mathbb{R}^n\rightarrow \mathbb{R}$ ist ein Vektor, wobei jeder Eintrag die partielle Ableitung der entsprechenden Komponente ist, also
\begin{equation*} \begin{split} \nabla f(x) = \left( \begin{array}{c}
\partial_{x_1} f(x) \\
\partial_{x_2} f(x) \\
... \\
\partial_{x_n} f(x)
\end{array}
\right).
\end{split} \end{equation*} 
\end{Def}

Der Gradient zeigt immer in Richtung des steilsten Anstiegs einer Funktion. Der negative Gradient in die Richtung des steilsten Abstiegs, weshalb wir das Gradientenabstiegsverfahren verwenden können, um das Minimum einer mehrdimensionalen Funktion zu finden. Wichtig für das Gradientenabstiegsverfahren ist das folgende Lemma

\begin{Lem}
\label{lem:gradientabstieg}
F\"ur eine differenzierbare Funktion $f$ ist es immer moeglich einen Skalar $\lambda>0$ zu finden, sodass
\begin{equation}
\label{lambda-ungleichung}
f(x - \lambda \nabla f(x)) \leq f(x)
\end{equation} 
\end{Lem}


\begin{proof}
Wir betrachten den nicht-trivialen Fall in dem $\nabla f(x) \neq 0$. 
Die zu zeigende Ungleichung \eqref{lambda-ungleichung} laesst sich umschreiben in die folgende Form
\begin{align*}
&f(x - \lambda \nabla f(x))\\
 = \, &f(x) - \lambda \langle \nabla f(x), \nabla f(x)\rangle + r(\lambda \nabla f(x)) \\
= \, &f(x) - \lambda( \Vert\nabla f(x)\Vert^2 - \frac{r(h)}{\lambda}) \overset{!}{\leq} f(x)
\end{align*}
Es ist hinreichend zu zeigen, dass ein $\lambda>0$ existiert, sodass
\begin{equation}
\label{eq:positiveterm}
\Vert\nabla f(x)\Vert^2 - \frac{r(\lambda \nabla f(x))}{\lambda} >0.
\end{equation}
  Dies ist dadurch gegeben, dass 
\begin{align*}
\lim_{\lambda\to 0} \frac{r(\lambda \nabla f(x))}{\lambda} &= \lim_{\lambda \to 0}\Vert\nabla f(x)\Vert \frac{r(\lambda \nabla f(x))}{\lambda \Vert\nabla f(x)\Vert}= 0
\end{align*}
Die letzte Gleichung folgt durch die Bedingung an das Restglied, dass $\lim_{h\to 0}\frac{r(h)}{h} = 0$. Durch die Definition eines Grenzwerts wissen wir also, dass ein $\lambda^*$ existiert, sodass $\left| \frac{r(\lambda^* \nabla f(x))}{\lambda^* \Vert\nabla f(x)\Vert} \right| \leq \Vert\nabla f(x)\Vert$. Zusammenfassend ist die Positivitaet des Terms \eqref{eq:positiveterm} damit gezeigt, d.h. es gilt fuer das gerade gewaehlte $\lambda^*$, dass
\begin{align*}
&\Vert\nabla f(x)\Vert^2 - \frac{r(\lambda^* \nabla f(x))}{\lambda^*} \\
\geq \, &\Vert\nabla f(x)\Vert \left( \Vert \nabla f(x)\Vert - \frac{r (\lambda^* \nabla f(x))}{\lambda^* \Vert \nabla f(x)\Vert} \right) \\
\geq \, &\Vert \nabla f(x)\Vert \left(\Vert\nabla f(x)\Vert - \left| \frac{r (\lambda^* \nabla f(x))}{\lambda^* \Vert \nabla f(x)\Vert}\right| \right) \geq 0. 
\end{align*}
\end{proof}




%% Das Ganze kann man noch weiter umschreiben, indem man das Skalarprodukt bildet: \\ \\
%% $f(x) + \lambda \langle \nabla f(x), \nabla f(x) \rangle + r(h) \leq f(x)$ \\ \\
%% Anschließend kann man von der Regel Gebrauch machen, dass sich ein Skalarprodukt, das in der Form $\langle u,u \rangle$ vorliegt, in die Ausprägung der Euklidischen Norm von $||u||^2$ umgewandelt werden kann. \\
%% Daraus folgt: \\ \\
%% $f(x) + \lambda \cdot || \nabla f(x)||^2 + r(h) \leq f(x)$ \\ \\
%% Um nun die obige Ungleichung (1) zu beweisen, muss man zeigen, dass \\ $\lambda ||\nabla f(x)||^2 + r(h) \leq 0$ ist.\\
%% Dafür wird $\lambda$ ausgeklammert. Anschließend erhält man:

%% \begin{equation} \lambda (||\nabla f(x)||^2 + \frac{r(h)}{\lambda} \end{equation}

%% Da $||\nabla f(x)||$ quadriert wird, wird dieser Term automatisch positiv. Da man den gesamten Term aber negativ haben will, wählt man $\lambda < 0.$ Schließlich kennt jeder die Regel, dass ein Produkt negativ wird, falls eine ungerade Zahl von Faktoren negativ ist. Da wir $\lambda$ negativ gewählt haben, muss \\ $||\nabla f(x)||^2$$(2) \lambda (||\nabla f(x)||^2 + \frac{r(h)}{\lambda}$ positiv sein.\\
%% Bei $||\nabla f(x)||^2||$ stellt dies aus den eben genannten Gr\"unden kein Problem dar. Deswegen muss nur noch gezeigt werden, dass es mindestens einen Wert von $r(h)$ gibt, der gr\"oßer oder gleich 0 ist. Ansonsten würde der Wert in der Klammer negativ werden k\"onnen und damit die unspr\"ungliche Aussage (1) widerlegen.\\
%% Damit wir zeigen k\"onnen, dass $r(h) \geq 0$ ist, formten wir zuerst den Term (2) um:\\

%% $\lambda (||\nabla f(x)||^2 + \frac{r(h)}{\lambda})$ \\

%% $\Leftrightarrow \lambda(||\nabla f(x)||^2 + \frac{r(\lambda \nabla f(x))}{\lambda})$ \\

%% Bei dieser Umformung wurde h wieder mit $\lambda\nabla f(x)$ ersetzt.

%% $\leftrightarrow \lambda ||\nabla f(x)|| \cdot (||\nabla f(x)|| + \frac{r(\lambda \nabla f(x))}{\lambda || f(x)||})$


%% Durch diese Umformungen schlussfolgerten wir, dass f\"ur die Funktion $h(x)$ folgendes gelten muss:\\
%% \begin{equation} h(\lambda)=||\nabla f(x)|| + \frac{r(\lambda\nabla f(x))}{\lambda||\nabla f(x)||} \geq 0 \end{equation}


%% Da in $||\nabla f(x)$ kein $\lambda$ steht ist dieser Term konstant. Gleichzeitig wird durch das Bilden der euklidischen Norm festgelegt, dass ein Wert größe/gleich null vorliegt. \\ %der Satz ist noch nicht wiklich schön formuliert
%% Deswegen muss nur noch gezeigt werden, dass dder zweite Summand ebenfalls positiv werden kann. Dazu bildeten wir den Grenzwert. \\ %Die Zeitformen stimmen sehr oft nicht
%% $\lim\limits_{h \rightarrow 0}{\frac{r(\lambda \nabla f(x))}{\lambda ||\nabla f(x)||}} = \frac{r(\lambda \nabla f(x))}{||\lambda  \nabla f(x)||}$ \\ %hier kommt noch kein h vor. Umschreiben?

%% Man kann $\lambda \nabla f(x)$ wieder durch h ersetzen, woraus folgt: \\
%% $\lim\limits_{h \rightarrow 0} \frac{r(h)}{h}$ \\

%% Der Grenzwert des Restbetrages strebt gegen Null. Daraus lässt sich schließen, dass es einen Punkt gibt, ab welchem die Ungleichung der Funktion $h(\lambda)$ (3) erfüllt ist und damit auch die anfängliche Aussage (1). \\ %ist hier h(\lambda) und (3) beies nötig?

\subsection{Summen-, Produkt- und Kettenregel}

Falls mehrere Funktonen miteinander verknüpft sind gelten folgende drei Regeln.
\begin{itemize}
\item Die \emph{Summenregel} besagt, dass der Gradient der Funktion $h(x)=f(x)+g(x)$ die Summe der Gradienten ist, das heißt
\begin{equation*} D(f + g)(x)=\text{D} f(x)+\text{D} g(x)\end{equation*}

\item Gegeben seien die differenzierbaren Funktionen $f:\mathbb{R}^k\rightarrow\mathbb{R}^n$, $g:\mathbb{R}^n\rightarrow\mathbb{R}^k$ und $h(x)=f(g(x))$. Die \emph{Kettenregel} besagt, dass
\begin{equation*}
\text{D}h(x) = \text{D}f(g(x)) \cdot \text{D} g(x).
\end{equation*}

\item Gegeben zwei differenzierbaren Funktionen $f:\mathbb{R}^m\rightarrow\mathbb{R}$ und $g:\mathbb{R}^m\rightarrow\mathbb{R}$, so gilt für das Produkt der Funktionen $h(x)=f(x) \cdot g(x)$ die \emph{Produktregel} \begin{equation*}\text{D}h(x)=f(x) \cdot \text{D}g(x)+\text{D}f(x) \cdot g(x).\end{equation*}
 \end{itemize}
Wir beweisen nun die Produktregel.
Da die Funktionen $f$ und $g$ differenzierbar sind gilt
\begin{align*}
  f(x+h) &= f(x) +  Df(x) h + r_{f}(h)\\
  g(x+h) &= f(x) + Dg(x) h + r_{g}(h)
\end{align*}
wobei $\lim_{h\rightarrow 0} r_f(h)/h = 0$ und $\lim_{h\rightarrow 0} r_g(h)/h = 0$.
Wir schreiben zuerst die Ableitung von $h(x+v)$ als Kombination aus den Ableitungen von $f(x)$ und $g(x)$ auf, also $h(x+v) =  f(x+v)\cdot g(x+v)$.
Das Produkt wird ausmultipliziert und wir erhalten
\begin{equation*}
\begin{split} h(x+v) & = f(x) \cdot g(x)  + f(x)  \cdot D_{g}(x) \cdot v \\ & + f(x) \cdot r_{g}(v) + Df(x) \cdot v \cdot g(x) \\ & +  Df(x) \cdot v \cdot D_{g}(x) \cdot v  + Df(x) \cdot v \cdot r_{g}(v) \\ & +  r_{g}(v) \cdot g(x) + r_{g}(x) \cdot Dg(x) \cdot v \\ & + r_{g}(v) \cdot r_{g}(v) \end{split} \end{equation*}   %Die Formel passt nicht in eine Zeile. Umbruch?

Von den ganzen Summanden müsssen letztendlich alle eliminiert werden, bis auf 
\begin{equation*} f(x) \cdot g(x) + f(x) \cdot Dg(x) \cdot v + Df(x) \cdot v \cdot g(x) \end{equation*} 
Dafür müssen wir zeigen, dass die übrigen Summanden gegen Null gehen. Um das zu erreichen bilden wir den Grenzwert
\begin{align*}
\lim \limits_{v \rightarrow 0} &||v||^{-1}(||f(x) \cdot r_{g}v + Df(x) \cdot v \cdot Dg(x) \cdot v +{}\\
&\qquad + Df(x) \cdot v \cdot r_{g}(v) r_{f}(v) \cdot g(x) +{}\\
&\qquad + r_{g}(v) \cdot Dg(x) \cdot v + r_{g}(v) \cdot r_{f}(v)||)
\end{align*}
Damit die Summanden einzeln behandelt werden können, wenden wir die Dreiecksungleichung an. Dann können wir von jedem Summanden einzeln den Limes bilden, um zu zeigen, dass der gesamte Grenzwert gegen Null geht. Dies ist aber nur der Fall, falls der Limes jedes Summanden gegen Null geht. Wir zeigen das exemplarisch für 

\begin{equation*}
\lim\limits_{v \rightarrow 0} \frac{||Df(x) \cdot r_{g}(v)||}{||v||}
\end{equation*}

$Df(x)$ ist eine Matrix mit einer Zeile und m Spalten in der jeweils ein Eintrag $\leq c$ steht. Daraus folgern wir:
\begin{align*}
&\lim\limits_{v \rightarrow 0} \frac{m \cdot c \cdot ||v|| \cdot ||r_{g}(v)||}{||v||}\\
&\qquad= m \cdot c \cdot \lim\limits_{v \rightarrow 0} \frac{ ||v|| \cdot ||r_{g}(v)||}{||v||} = 0
\end{align*} 
Damit haben wir gezeigt, dass der Summand gegen Null geht. Ähnlich behandelt man auch die anderen Summanden und auch diese gehen alle gegen Null.
Die Produktregel ist damit bewiesen. 













