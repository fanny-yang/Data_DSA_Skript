\section{Analysis}
\authors{Franziska Wilfinger, Jan Fritz, Florentine Klepel}  

Analysis ist ein wichtiges Teilgebiet der Mathematik. Sie ist Grundlage für die Optimierung und somit auch von großer Bedeutung für das maschinelle Lernen (Machine Learning). Im Kurs haben wir uns insbesondere mit der Stetigkeit und Differenzierbarkeit von ein- und mehrdimensionalen Funktionen besch\"aftigt.

\subsection{Stetigkeit und Differenzierbarkeit}

Wir beginnen mit einigen theoretischen Grundlagen der Analysis.
\begin{Def} Die Zahl $L$ heißt \emph{Grenzwert} der Funktion $f:\mathbb{R}^n \to \mathbb{R}^m$ für $x$ gegen $a$, wenn für alle $\epsilon > 0$ ein $\delta > 0$ existiert, sodass für alle $\|x-a\| < \delta$ gilt $\|f(x)-L\|<\epsilon$
\end{Def}

Der Grenzwert ist notwendig f\"ur die Definition von Stetigkeit und Differenzierbarkeit, die Grundbausteine der Analysis und Optimierung. 
\begin{Def}
Eine Funktion $f: D \subseteq \mathbb{R}^n\rightarrow \mathbb{R}^m$ hei\ss t \emph{stetig}, wenn $\lim_{w\rightarrow x}f(w)=f(x)$ auf ihrem gesamten Definitionsbereich gilt, das heißt wenn sie stetig in $x$ für alle $x\in D$ ist.
\end{Def}
\begin{Def}
\label{def:diffbar}
Eine Funktion $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$ ist an einer Stelle $x$ \emph{differenzierbar}, wenn eine lineare Abbildung $ l_ x (h)$ und ein Restglied $r(h)$ existieren, sodass $f(x+h)=f(x)+l_{x}(h)+r(h)$, wobei $\lim_{h\rightarrow 0} \|r(h)\|/\|h\|= 0$ gilt.
\end{Def}

Da die lineare Abbildung $l_x(h)$ als Matrix-Vektor Produkt geschrieben werden kann (siehe Abschnitt~\ref{sec:linabb}), d.h.
\begin{equation*}
l_x(h) = \text{D} f(x) h
\end{equation*}
wobei $\text{D} f(x) \in \mathbb{R}^{m \times n}$ eine Matrix abh\"angig von $x$ ist, der Stelle an der abgeleitet wird. Diese Matrix nennt man die mehrdimensionale \emph{Ableitung} von $f$ an der Stelle $x$.  Im Spezialfall $m=1$ ist diese Matrix ein Vektor, der Gradient genannt wird (siehe Abschnitt~\ref{sec:gradient}.

Wir betrachten nun eine einfache Folgerung aus diesen Definitionen.

\begin{Lem}
Eine Funktion $f:\mathbb{R} \rightarrow \mathbb{R}$, welche differenzierbar an der Stelle $x \in \mathbb{R}$ ist, ist dort auch stetig.
\end{Lem}

\begin{proof}
Wir überlegen zuerst, welche Bedingungen gegeben sind. Nach Definition~\ref{def:diffbar} ist eine Funktion an der Stelle $x$ differenzierbar, wenn $f(x+h)=f(x)+l_x(h)+ r(h)$  mit $\lim_{h \rightarrow 0} r(h)/h= 0$. Um die Stetigkeit nachzuweisen müssen wir zeigen, dass $\lim_{w \rightarrow x} f(w)=f(x)$. Mit $w = x+h$ ist dies äquivalent zu $\lim_{h \rightarrow 0} f(x)+l_x(h)+r(h)=f(x)$. Es genügt also zu zeigen, dass  $l_x(h)$ und $r(h)$ gegen Null gehen.
\begin{itemize}
  \item Würde $r(h)$ nicht gegen Null gehen, so würde $r(h)/h$ nicht gegen Null gehen. Das steht im Widerspruch zur Differenzierbarkeit.
  \item Der lineare Term $l_x(h)$ kann auch als $f'(x) \cdot h$ dargestellt werden. Da $h$ gegen Null geht, folgt auch $l_x(h)\rightarrow 0$.
\end{itemize}
Damit ist gezeigt, dass $\lim_{w\rightarrow x} f(w) = f(x)$.
\end{proof}

\subsection{Gradient}
\label{sec:gradient}
\begin{Def}
Der \emph{Gradient} $\nabla f(x)$ einer Funktion $f:\mathbb{R}^n\rightarrow \mathbb{R}$ ist ein Vektor, wobei jeder Eintrag der partiellen Ableitung von $f$ nach der entsprechenden Komponente entspricht, also
\begin{equation*} \begin{split} \nabla f(x) = \left( \begin{array}{c}
\partial_{x_1} f(x) \\
\partial_{x_2} f(x) \\
... \\
\partial_{x_n} f(x)
\end{array}
\right).
\end{split} \end{equation*} 
\end{Def}

Der Gradient zeigt immer in Richtung des steilsten Anstiegs einer Funktion, der negative Gradient in die Richtung des steilsten Abstiegs. Deshalb k\"onnen wir das Gradientenabstiegsverfahren verwenden, um das Minimum einer mehrdimensionalen Funktion zu finden. Wichtig für das Gradientenabstiegsverfahren ist das folgende Lemma

\begin{Lem}
\label{lem:gradientabstieg}
F\"ur eine differenzierbare Funktion $f$ ist es immer m\"oglich, ein Skalar $\lambda>0$ zu finden, sodass
\begin{equation}
\label{lambda-ungleichung}
f(x - \lambda \nabla f(x)) \leq f(x)
\end{equation} 
\end{Lem}


\begin{proof}
Wir betrachten den nicht-trivialen Fall in dem $\nabla f(x) \neq 0$. 
Die zu zeigende Ungleichung \eqref{lambda-ungleichung} l\"asst sich umschreiben in die folgende Form
\begin{align*}
&f(x - \lambda \nabla f(x))\\
 = \, &f(x) - \lambda \langle \nabla f(x), \nabla f(x)\rangle + r(\lambda \nabla f(x)) \\
= \, &f(x) - \lambda \left( \Vert\nabla f(x)\Vert^2 - \frac{r(h)}{\lambda}\right) \overset{!}{\leq} f(x)
\end{align*}
Es ist hinreichend zu zeigen, dass ein $\lambda>0$ existiert, sodass
\begin{equation}
\label{eq:positiveterm}
\Vert\nabla f(x)\Vert^2 - \frac{r(\lambda \nabla f(x))}{\lambda} >0.
\end{equation}
  Dies ist dadurch gegeben, dass 
\begin{align*}
\lim_{\lambda\to 0} \frac{r(\lambda \nabla f(x))}{\lambda} &= \lim_{\lambda \to 0}\Vert\nabla f(x)\Vert \frac{r(\lambda \nabla f(x))}{\lambda \Vert\nabla f(x)\Vert}= 0
\end{align*}
Die letzte Gleichung folgt durch die Bedingung an das Restglied, dass $\lim_{h\to 0}\frac{r(h)}{h} = 0$. Durch die Definition eines Grenzwerts wissen wir also, dass ein $\lambda^*$ existiert, sodass $\left| \frac{r(\lambda^* \nabla f(x))}{\lambda^* \Vert\nabla f(x)\Vert} \right| \leq \Vert\nabla f(x)\Vert$. Zusammenfassend ist die Positivit\"at des Terms \eqref{eq:positiveterm} damit gezeigt, d.h. es gilt f\"ur das gerade gew\"ahlte $\lambda^*$, dass
\begin{align*}
&\Vert\nabla f(x)\Vert^2 - \frac{r(\lambda^* \nabla f(x))}{\lambda^*} \\
\geq \, &\Vert\nabla f(x)\Vert \left( \Vert \nabla f(x)\Vert - \frac{r (\lambda^* \nabla f(x))}{\lambda^* \Vert \nabla f(x)\Vert} \right) \\
\geq \, &\Vert \nabla f(x)\Vert \left(\Vert\nabla f(x)\Vert - \left| \frac{r (\lambda^* \nabla f(x))}{\lambda^* \Vert \nabla f(x)\Vert}\right| \right) \geq 0. 
\end{align*}
\end{proof}



%% Das Ganze kann man noch weiter umschreiben, indem man das Skalarprodukt bildet: \\ \\
%% $f(x) + \lambda \langle \nabla f(x), \nabla f(x) \rangle + r(h) \leq f(x)$ \\ \\
%% Anschließend kann man von der Regel Gebrauch machen, dass sich ein Skalarprodukt, das in der Form $\langle u,u \rangle$ vorliegt, in die Ausprägung der Euklidischen Norm von $||u||^2$ umgewandelt werden kann. \\
%% Daraus folgt: \\ \\
%% $f(x) + \lambda \cdot || \nabla f(x)||^2 + r(h) \leq f(x)$ \\ \\
%% Um nun die obige Ungleichung (1) zu beweisen, muss man zeigen, dass \\ $\lambda ||\nabla f(x)||^2 + r(h) \leq 0$ ist.\\
%% Dafür wird $\lambda$ ausgeklammert. Anschließend erhält man:

%% \begin{equation} \lambda (||\nabla f(x)||^2 + \frac{r(h)}{\lambda} \end{equation}

%% Da $||\nabla f(x)||$ quadriert wird, wird dieser Term automatisch positiv. Da man den gesamten Term aber negativ haben will, wählt man $\lambda < 0.$ Schließlich kennt jeder die Regel, dass ein Produkt negativ wird, falls eine ungerade Zahl von Faktoren negativ ist. Da wir $\lambda$ negativ gewählt haben, muss \\ $||\nabla f(x)||^2$$(2) \lambda (||\nabla f(x)||^2 + \frac{r(h)}{\lambda}$ positiv sein.\\
%% Bei $||\nabla f(x)||^2||$ stellt dies aus den eben genannten Gr\"unden kein Problem dar. Deswegen muss nur noch gezeigt werden, dass es mindestens einen Wert von $r(h)$ gibt, der gr\"oßer oder gleich 0 ist. Ansonsten würde der Wert in der Klammer negativ werden k\"onnen und damit die unspr\"ungliche Aussage (1) widerlegen.\\
%% Damit wir zeigen k\"onnen, dass $r(h) \geq 0$ ist, formten wir zuerst den Term (2) um:\\

%% $\lambda (||\nabla f(x)||^2 + \frac{r(h)}{\lambda})$ \\

%% $\Leftrightarrow \lambda(||\nabla f(x)||^2 + \frac{r(\lambda \nabla f(x))}{\lambda})$ \\

%% Bei dieser Umformung wurde h wieder mit $\lambda\nabla f(x)$ ersetzt.

%% $\leftrightarrow \lambda ||\nabla f(x)|| \cdot (||\nabla f(x)|| + \frac{r(\lambda \nabla f(x))}{\lambda || f(x)||})$


%% Durch diese Umformungen schlussfolgerten wir, dass f\"ur die Funktion $h(x)$ folgendes gelten muss:\\
%% \begin{equation} h(\lambda)=||\nabla f(x)|| + \frac{r(\lambda\nabla f(x))}{\lambda||\nabla f(x)||} \geq 0 \end{equation}


%% Da in $||\nabla f(x)$ kein $\lambda$ steht ist dieser Term konstant. Gleichzeitig wird durch das Bilden der euklidischen Norm festgelegt, dass ein Wert größe/gleich null vorliegt. \\ %der Satz ist noch nicht wiklich schön formuliert
%% Deswegen muss nur noch gezeigt werden, dass dder zweite Summand ebenfalls positiv werden kann. Dazu bildeten wir den Grenzwert. \\ %Die Zeitformen stimmen sehr oft nicht
%% $\lim\limits_{h \rightarrow 0}{\frac{r(\lambda \nabla f(x))}{\lambda ||\nabla f(x)||}} = \frac{r(\lambda \nabla f(x))}{||\lambda  \nabla f(x)||}$ \\ %hier kommt noch kein h vor. Umschreiben?

%% Man kann $\lambda \nabla f(x)$ wieder durch h ersetzen, woraus folgt: \\
%% $\lim\limits_{h \rightarrow 0} \frac{r(h)}{h}$ \\

%% Der Grenzwert des Restbetrages strebt gegen Null. Daraus lässt sich schließen, dass es einen Punkt gibt, ab welchem die Ungleichung der Funktion $h(\lambda)$ (3) erfüllt ist und damit auch die anfängliche Aussage (1). \\ %ist hier h(\lambda) und (3) beies nötig?

\subsection{Summen-, Produkt- und Kettenregel}

Falls mehrere Funktionen miteinander verknüpft sind gelten folgende drei Regeln f\"ur die mehrdimensionale Ableitung:
\begin{itemize}
\item Gegen seien zwei differenzierbare Funktionen $f:\mathbb{R}^m\rightarrow\mathbb{R}$ und $g:\mathbb{R}^m\rightarrow\mathbb{R}$. Die \emph{Summenregel} besagt, dass der Gradient (also Ableitung) der Summe beider Funktionen $h = f + g$ die Summe der Gradienten ist, das heißt
\begin{equation*} \text{D}(f + g)(x)=\text{D} f(x)+\text{D} g(x)\end{equation*}

\item Gegeben seien die differenzierbaren Funktionen $f:\mathbb{R}^k\rightarrow\mathbb{R}^n$, $g:\mathbb{R}^n\rightarrow\mathbb{R}^k$ und $h(x)=f(g(x))$. Die \emph{Kettenregel} besagt, dass
\begin{equation*}
\text{D}h(x) = \text{D}f(g(x)) \cdot \text{D} g(x).
\end{equation*}

\item Gegeben seien zwei differenzierbare Funktionen $f:\mathbb{R}^m\rightarrow\mathbb{R}$ und $g:\mathbb{R}^m\rightarrow\mathbb{R}$, so gilt für das Produkt der Funktionen $h(x)=f(x) \cdot g(x)$ die \emph{Produktregel} \begin{equation*}\text{D}h(x)=f(x) \cdot \text{D}g(x)+\text{D}f(x) \cdot g(x).\end{equation*}
 \end{itemize}
Wir beweisen nun die Produktregel.
Da die Funktionen $f$ und $g$ differenzierbar sind gilt
\begin{align*}
  f(x+h) &= f(x) +  \text{D}f(x) h + r_{f}(h)\\
  g(x+h) &= f(x) + \text{D}g(x) h + r_{g}(h)
\end{align*}
wobei $\lim_{h\rightarrow 0} r_f(h)/h = 0$ und $\lim_{h\rightarrow 0} r_g(h)/h = 0$.
Wir schreiben zuerst die Ableitung von $h(x+v)$ als Kombination aus den Ableitungen von $f(x)$ und $g(x)$ auf, also $h(x+v) =  f(x+v)\cdot g(x+v)$.
Das Produkt wird ausmultipliziert und wir erhalten
\begin{equation*}
\begin{split} h(x+v) & = f(x) \cdot g(x)  + f(x)  \cdot \text{D}_{g}(x) \cdot v \\ & + f(x) \cdot r_{g}(v) + \text{D}f(x) \cdot v \cdot g(x) \\ & +  \text{D}f(x) \cdot v \cdot \text{D}_{g}(x) \cdot v  + \text{D}f(x) \cdot v \cdot r_{g}(v) \\ & +  r_{f}(v) \cdot g(x) + r_{f}(x) \cdot \text{D} g(x) \cdot v \\ & + r_{f}(v) \cdot r_{g}(v) \end{split} \end{equation*}   %Die Formel passt nicht in eine Zeile. Umbruch?

Von den ganzen Summanden müssen letztendlich alle eliminiert werden, bis auf 
\begin{equation*} f(x) \cdot g(x) + f(x) \cdot \text{D}g(x) \cdot v + \text{D}f(x) \cdot v \cdot g(x) \end{equation*} 
Dafür müssen wir zeigen, dass die übrigen Summanden gegen Null gehen. Um das zu erreichen, bilden wir den Grenzwert
\begin{align*}
\lim \limits_{v \rightarrow 0} &||v||^{-1}(||f(x) \cdot r_{g}v + \text{D}f(x) \cdot v \cdot \text{D}g(x) \cdot v +{}\\
&\qquad + \text{D}f(x) \cdot v \cdot r_{g}(v) r_{f}(v) \cdot g(x) +{}\\
&\qquad + r_{g}(v) \cdot \text{D}g(x) \cdot v + r_{g}(v) \cdot r_{f}(v)||)
\end{align*}
Damit die Summanden einzeln behandelt werden können, wenden wir die Dreiecksungleichung an. Dann können wir von jedem Summanden einzeln den Limes bilden, um zu zeigen, dass der gesamte Grenzwert gegen Null geht. Dies ist aber nur der Fall, falls der Limes jedes Summanden gegen Null geht. Wir zeigen dies exemplarisch für 

\begin{equation*}
\lim\limits_{v \rightarrow 0} \frac{||Df(x) \cdot r_{g}(v)||}{||v||}
\end{equation*}

$Df(x)$ ist eine Matrix mit einer Zeile und m Spalten in der jeweils ein Eintrag $\leq c$ steht. Daraus folgern wir:
\begin{align*}
&\lim\limits_{v \rightarrow 0} \frac{m \cdot c \cdot ||v|| \cdot ||r_{g}(v)||}{||v||}\\
&\qquad= m \cdot c \cdot \lim\limits_{v \rightarrow 0} \frac{ ||v|| \cdot ||r_{g}(v)||}{||v||} = 0
\end{align*} 
Damit haben wir gezeigt, dass der Summand gegen Null geht. Ähnlich behandelt man auch die anderen Summanden und auch diese gehen alle gegen Null.
Die Produktregel ist damit bewiesen. 













