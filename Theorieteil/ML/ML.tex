\section{Machine Learning}
\author{Farhadiba Mohammed, Dennis Kempf, David Steinmann}
Machine Learning ist ein Konzept, bei dem der Computer durch Algorithmen aus Daten lernt, gewisse Probleme selbstständig zu lösen. Dabei erkennt der Computer nach einer gewissen Lernphase Gesetzmä\ss igkeiten, durch die er dann in der Lage ist, mit neuen, ihm unbekannte Datensätze umzugehen. Dabei werden verschiedene Modelle genutzt, doch zunächst einmal sollten wir uns mit den grundsätzlichen Begriffen Klassifikation und Regression vertraut machen.

\subsection{Klassifikation und Regression}
\author{David Steinmann}
Bei Klassifikation und Regression handelt es sich um zwei verscheidene Möglichkeiten zur Problemlösung im Bereich Machine Learning.

\paragraph{Klassifikation.}
Bei der Klassifikation sorgt der Algorithmus am Ende dafür, dass der Datensatz vom Computer in verschiedene Klassen eingeteilt wird. Die Klassen müssen vorher vom Mensch festgelegt werden. Der Computer klassifiziert die Datensätze dann anhand der Attribute (sogenannten \emph{features}) des entsprechenden Datensatzes. Die klassifizierten Datensätze enthalten dann meistens eine weitere Dimension, in der die Klasse anhand einer Zahl gespeichert ist.

\paragraph{Regression.}
Bei der Regression geht es darum, den entsprechenden Datensätzen am Ende einen festen Wert zuzuordnen. Der Unterschied zur Klassifikation besteht darin, dass der zugeordnete Wert nicht für eine Klasse steht, sondern eine konkrete reelle Zahl beinhaltet. Ein Beispiel wäre der Preis von einer Wohnung. Diese wurde mit vielen verscheidenen Attributen in die Datenbank eingespeichert (zum Beispiel Größe, Lage, ob sie von einem) der Endwert wäre dann zum Beispiel der konkrete Preis und nicht eine Klassifizierung in \enquote{teuer} , \enquote{mittelteuer}, \enquote{billig}. 

\subsection{Support Vector Machines}
\author{David Steinmann}
Support Vector Machines oder kurz SVMs sind ein Modell im Machine Learning.
Dabei werden für alle Datensätze zuerst gewisse Features festgelegt. Diese Features entsprechen den Attributen des Datensatzes. Je nach Problem kann es unterschiedlich viele Features geben, doch allgemein kann man sagen, dass mit der Anzahl der Features auch die Genauigkeit des Programms wächst.

Diese Features werden normalerweise in einem Vektor $x_i\in \mathbb{R}^m$ für den entsprechenden Datensatz $i = 1, \dots, n$ gespeichert.
Zusätzlich hat jeder Datensatz noch einen weiteren Wert $y_{i} \in \mathbb{R}$, welcher das \enquote{Ergebnis} oder \enquote{Label} des Datensatzes beschreibt. Meist betrachtet man zwei Klassen $y_i \in \{-1, +1\}$.

% Die Features und das Label zusammen beschreiben einen Punkt im $(m+1)$-dimensionalen Koordinatensystem, da zu den $m$ Dimensionen von $x$ noch die eine Dimension von $y$ dazukommt.

Durch die Features ist es dann möglich den Datensätzen einen bestimmten Vektor im $m$-dimensionalen Koordinatensystem zuzuordnen. Diese können dann auf Grund ihrer räumlichen Anordnung klassifiziert werden. Dabei soll der Abstand von beiden Gruppen von Punkten zu dem Trennobjekt maximal sein. Dadurch kann die Klassifizierung optimal durchgeführt werden. Andernfalls würden schon geringe Abweichungen von den Test-Datensätzen reichen, das ein Datensatz falsch klassifiziert würde (siehe Abb. \ref{SVM1}). 


\begin{dsafigure}
\begin{center}
	\label{SVM1}
	\includegraphics[width=0.35\textwidth]{\media Figure_SimpleSVM}
	\caption{Da der Abstand zwischen den unterschiedlich klassifizierten Datensätzen maximiert werden soll, gilt die durchgezogene und nicht die gestrichelte Linie als Trennelement.}
	\end{center}
\end{dsafigure}


Für die mathematische Formulierung des zugehörigen Optimierungsproblems stellen wir zunächst fest dass $2/\norm{w}$ der Abstand der Ebenen $\langle w, x_i\rangle - b = 1$ und $\langle w, x_i\rangle - b = -1$ ist (siehe Abb. \ref{SVM2}). Man nennt diesen Abstand auch den \emph{margin} der Klassifikationsebene $\langle w, x\rangle - b = 0$, $1/\norm{w}$ ist der maximal Abstand der Testdatenpunkte von der Ebene. Das folgende Programm berechnet somit eine Klassifikationsebene mit bester Trennung:
\begin{align*}
	\underset{w,b}{\operatorname{maximiere}} \quad & \frac{2}{\norm{w}} \\
	\operatorname{sodass} \quad &y_{i}(\langle w,x_{i} \rangle -b) \geq 1 \quad \forall  i = 1,\dots ,n
	 %\label{SVMProblemMax}
\end{align*}
Dieses Problem is äquivalent zu
\begin{align*}
		\underset{w,b}{\operatorname{minimiere}} \quad & \norm{w} \\
		\operatorname{sodass} \quad &y_{i}(\langle w,x_{i} \rangle -b) \geq 1 \quad \forall i = 1,\dots,n
\end{align*}
Die Klassifikation neuer Datenpunkte erfolgt dann nach der Regel
\begin{align*}
	y_{i} =& 
        \begin{cases}
1 &\text{falls} \quad \langle w,x_{i} \rangle - b \geq 0 \\
-1 &\text{falls} \quad \langle w,x_{i} \rangle - b \leq 0
\end{cases}
	%\label{KlassifikationSVM}
\end{align*}
Nachdem das Problem optimiert wurde, ist der Computer in der Lage, weitere Daten einzuordnen.
Dennoch kann es bei dieser Art von SVMs zu einem Problemen kommen: Wenn die Datensätze nicht linear separierbar sind, das heißt es ist nicht möglich, die beiden Datensätze mit einem linearen Element zu trennen. Dann ist obiges Optimierungsproblem nicht lösbar.

\begin{dsafigure}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{\media Figure_SVM.pdf}
		\caption{Links und rechts zur Trenngeraden befinden sich die parallelen Grenzen (gestrichelte Geraden). Ziel der Optimierung ist es, den Abstand zwischen den Grenzen zu maximieren, um den Normalenvektor $w$ zu bestimmen.}
		\label{SVM2}
	\end{center}
\end{dsafigure}

\paragraph{Soft Margin SVM.}
Die Soft Margin SVMs können mit diesem Problem umgehen.
Sie sind im Prinzip wie herkömmliche SVMs aufgebaut, nur dass sie eine gewisse Fehlertoleranz besitzen. Diese kommt durch die folgende Modifikation des Optimierungsproblems zustande:
\begin{align*}
	\underset{w,b}{\operatorname{minimiere}} \quad & \norm{w} + C\sum_{i}{z_{i}} \\
	\operatorname{sodass} \quad & y_{i}(\langle w,x_{i} \rangle - b) \geq 1 - z_{i} \quad \forall i = 1,...,n
	%\label{eq: SoftMarginSVMs}
\end{align*}
Dabei steht $z_{i}$ für die Grö\ss e des Fehlers des Punktes $x_{i}$ und $C$ ist eine Konstante, deren Grö\ss e bestimmt, wie stark die Fehler gewichtet werden. Da die gesamte Gleichung minimiert werden soll sorgt ein hohes $C$ dafür, dass das Problem schwerer optimiert werden kann. Die Konstante muss vom Mensch gewählt werden, je nachdem, wie schwer Fehler gewichtet werden sollen.
Um die optimale Größe von $C$ für das entsprechende Problem zu finden wird ein Verfahren namens \emph{crossvalidation} genutzt. Dabei wird der Algorithmus mehrmals auf den gleichen Testdatensatz angewendet, $C$ dabei aber variiert. Damit kann man herausfinden, bei welchem Wert von $C$ der Algorithmus optimal funktioniert.


\section{Neuronale Netze}
\author {Farhadiba Mohammed}

\subsection{Einführung}

Inspiriert von unserem Verständnis über das menschliche Gehirn benutzen neuronale Netze Lernalgorithmen, welche besonders für praktische Anwendungen geeignet sind.
Dazu zählen Spracherkennung, Objekterkennung in Bildern und die Fähigkeit individuell passende Produkte vorzuschlagen, die dem Kunden gefallen könnten. 
Ein neuronales Netz ist aus mehreren Schichten aufgebaut. Ausgangsschicht ist dabei, die Datenschicht, auf die ein oder mehrere sogenannte \emph{hidden layer} folgen. Als Ausgabewert erhält man schließlich einen Vektor, welcher eine Wahrscheinlichkeitsverteilung darstellt, die angibt wie wahrscheinlich es ist, dass das Ausgangsobjekt zu einer bestimmten Klasse gehört.

\begin{dsafigure}
	\begin{center}
		\includegraphics[width=0.35\textwidth]{\media Figure_NN.pdf}
		\caption{Ein vollständig verbundenes neuronales Netzwerk mit $i$ Eingängen und $k$ Ausgängen, bestehend aus $n$ Schichten mit jeweils $m$ \glqq Neuronen\grqq .}

		\label{FigNN}
	\end{center}
\end{dsafigure}

\subsection{Deep Learning}

Mit Deep Learning beschreibt man neuronale Netze, die über mehr als eine \enquote{versteckte Schicht} verfügen. Damit ist gemeint, dass sich zwischen Ein- und Ausgabeschicht weitere Schichten befinden. 

Um die Anzahl an Parametern zu vergrößern, werden diese auch über nicht-lineare Funktionen miteinander verknüpft (die Verkettung von linearen Funktion wäre wieder linear und besäße deshalb nur begrenzte Representationskraft). Jedoch kann eine zu hohe Anzahl an Parametern auch dazu führen, dass das Netzwerk \enquote{overfitted}, das heißt zu sehr an den Trainingsdatensatz angepasst wird. Dann kann das Netzwerk neue Daten nicht mehr korrekt klassifizieren.

Wie wir in der Abbildung \ref{FigNN} sehen können ist ein typisches \emph{fully connected} neuronales Netz abgebildet. Als Basis findet man unten die Datenschicht mit ihren Eingabewerten in Form eines Vektors $(x_1 \dots x_i)$. Diese Werte werden nun mit einem jeweiligen Faktor $w$ multipliziert. Hierbei handelt es sich um ein Skalarprodukt. Das Ergebnis wird nun im ersten hidden layer abgespeichert. Darauf wird eine nicht-lineare Funktion angewendet. Für die $k$-te hidden layer bekommen wir
\begin{align*}
a_j^{(k)} &= f^{(k)} (\langle w_{j}^{(k)}, a^{(k-1)}\rangle)\\ &= f^{(k)} \left(\sum_{i=1}^{m^{(k-1)}} w_{ji}^{(k)},a_i^{(k-1)}\right)
\end{align*}
Das Ergebnis wird erneut mit einem Faktor $w$ multipliziert. Dieser Vorgang wiederholt sich so lange bis uns schließlich ein Vektor mit seinen Komponenten $(p_1 \dots p_k)$ zurückgegeben wird, welcher als Wahrscheinlichkeitsverteilung interpretiert werden kann.

Anhand eines konkreten Beispiels würde es folgendes bedeuten:
Nehmen wir an wir haben ein Bild und wollen ermitteln zu welcher Klasse Haus, Tisch oder Stuhl das daurauf abgebildete Objekt gehört. Unsere inputs wären demnach die Pixel des Bildes. Diese durchlaufen nun das Neuronale Netz und wir erhalten eine Wahrscheinlichkeitsverteilung als Rückgabewert, welche uns mitteilt, dass es am wahrscheinlichsten ist, dass das abgebildete Objekt ein Objekt der Klasse Stuhl ist. Demnach ist die Klassifizierung vollzogen.

\subsection{Convolutional Neural Networks}
\label{ml:cnn}
\begin{dsafigure}
  \centering
		\includegraphics[width=0.5 \textwidth]{\media cnn.png}
		\caption{Ein Convolutional Neural Network (CNN) (deutsch \enquote{faltendes neurales Netzwerk}) %mit vier Eingängen ($x_1, ..., x_4$) und zwei Ausgängen ($p_1, p_2$). Dazwischen befindet sich eine Schicht aus drei >>Neuronen<<, die als Filter wirkt.}
		}
		\label{FigConvNN}
\end{dsafigure}
\noindent Bei \emph{convolutional networks} handelt es sich um spezielle neuronale Netze, die besonders für die Bilderkennung geeignet sind.

\begin{dsafigure}
  \centering
		\includegraphics[width=0.4 \textwidth]{\media ConvolutionalNN.png}
		\caption{Ein Convolutional NN mit einer Schicht aus neun \enquote{Neuronen} dazwischen, die als Filter wirkt. }
		\label{FigConvNN}
\end{dsafigure}

Wie in Fig.~\ref{FigConvNN} gezeigt wird der existierende Filter $w$ über die Eingabematrix geschoben. Dabei wird das Skalarprodukt berechnet und in der Endmatrix als Ergebnis festgehalten. Daraufhin wird der Filter immer um eine Stelle weiter nach rechts in der Eingabematrix verschoben und das neue Skalarprodukt berechnet bis das Ende der Eingabematrix erreicht wurde.

\subsection{SVMs als Neuronale Netze}

Wir haben bereits SVMs kennengelernt, die beschrieben werden durch das Optimierungsproblem
\begin{align*}
\underset{b,w,z}{\text{minimiere}} &\qquad\norm{w} + C  \sum_{i=1}^{n} z_i\\
\text{sodass} &\qquad y_i (\langle w,x \rangle - b) \geq 1 - z_i \text { mit } z_i \geq 0
\end{align*}
Die Nebenbedingung lässt sich umschreiben als
\begin{align*}
z_i \geq 1- y_i (\langle w,x \rangle - b), z_i \geq 0
\end{align*}
Da über $z_i$ minimiert wird und $C$ positiv ist, nehmen die Variablen $z_i$ immer die Grenzen an, wir bekommen also

\begin{align*}
\min &\norm{w} + C \cdot \sum_{i=1}^{n} \max (0, 1-y_i (\langle w,x_i\rangle-b) \\
 = &\norm{w} + C \sum_{i=1}^{n} \ell ( 1-y_i (\langle w,x_i\rangle)-b) \\
=&\norm{w} + C \sum_{i=1}^{n} \ell (1 - y_i a_i)
\end{align*}
Charakteristisch für dieses Neuronale Netz ist, dass es nur einen Layer besitzt.

\subsection{Lineare Regression}

Analog kann auch die lineare Regression in Form von Neuronalen Netzen ausgedrückt werden:
\begin{align*}
\min\limits_{w,b} \ & \frac{1} {2} \sum_{i=1}^{n} (a_i -y_i)^2, \quad a_i = \langle w,x_i\rangle -b
\end{align*}

\subsection{MNIST}

\begin{dsafigure}
	\begin{center}
		\includegraphics[width=0.4 \textwidth]{\media mnistExamples.png}
		\caption{MNIST Beispiele}
		\label{FigConvNN}
	\end{center}
\end{dsafigure}

MNIST ist ein freier Datensatz, der es erlaubt, SVMs und Neuronalen Netze zur Zeichenerkennung zu trainieren. Wir haben den Datensatz im Kurs verwendet, um unsere Modelle zu trainieren (siehe Fig.~\ref{FigConvNN}).



