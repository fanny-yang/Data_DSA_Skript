%\newcommand{\media}{../../media/}


\section{Machine Learning}
\author{Farhadiba Mohammed, Dennis Kempf, David Steinmann}
Machine Learning ist ein Konzept, bei dem der Computer durch Algorythmen in der Lage ist Probleme selbstständig zu lösen. Dabei erkennt der Computer nach einer gewissen Lernphase Gesetzmässigkeiten, durch die er dann in der Lage ist, neue, ihm unbekannte Datensätze bezüglich eines bestimmten Problems zu lösen.

Dass der Computer die Gesetzmässigkeiten erkennen kann, werden verschiedene Systeme genutzt, doch zunächst einmal sollten wir uns mit den Begriffen Klassifikation und Regression vertraut machen.


\subsection{Klassifikation und Regression}
\author{David Steinmann}
Bei Klassifikation und Regression handelt es sich um zwei verscheidene Arten von Problemen, beziehungsweise Möglichkeiten zur Problemlösung im Bereich Machine Learning.

\subsubsection{Klassifikation}
Bei der Klassifikation sorgt der Algorythmus am Ende dafür, dass der Datensatz klassifiziert wird. Das heißt, der Datensatz wird vom Computer in verschiedene Klassen eingeteilt. Die Klassen müssen vorher vom Mensch entschieden werden. Der Computer klassifiziert die Datensätze dann anhand der Attribute des entsprechenden Datensatzes. Die klassifizierten Datensätze enthalten dann meistens eine weitere Dimension, in der die Klassifizierung anhand einer Zahl gespeichert ist.

\subsubsection{Regression}
Bei der Regression geht es darum, den entsprechenden Datensätzen am Ende einen festen Wert zuzuordnen. Der Unterschied zur Klassifikation besteht darin, dass der zugeordnete Wert nicht für eine Klasse steht, sondern ein konkretes Ergebnis beinhaltet. Ein Beispiel wäre der Preis von einer Wohnung. Diese wurde mit vielen verscheidenen Attributen in die Datenbank eingespeichert (z. B. Größe, Lage ...) der Endwert wäre dann zum Beispiel der konkrete Preis und nicht eine Klassifizierung in "`teuer"', "`mittelteuer"', "`billig"'. 
%Noch die Mathematische Regression (methode der kleinsten Quadrate)

\subsection{Support Vector Machines}
\author{David Steinmann}
Support Vector Machines oder kurz SVMs sind ein Konzept zum Lösen von Machine Learning Problemen.
Dabei werden für alle Datensätze zürst gewisse Features festgelegt. Diese Features entsprechen den Attributen des Datensatzes. Je nach Problem kann es unterschiedlich viele Features geben, doch allgemein kann man sagen, dass, je mehr Features es sind die Genauigkeit des Programms genaür wird.

Diese Features werden normalerweise in einem Vektor für den entsprechenden Datensatz gespeichert.
Dieser Vektor wird $x_{i}$ genannt wobei $i = 1, ..., n$ den Datensatz beschreibt mit  $x \in  \mathbb{R}^m$.
Zusätzlich hat jeder Datensatz noch einen weiteren Wert, welcher das >>Ergebnis<< des Datensatzes beschreibt, der $y_{i} \in \mathbb{R}$ oder auch >>Label<< genannt wird.

Die Features und das Label zusammen beschreiben einen Punkt im n-dimensionalen Koordinatensystem, wobei $n = m + 1$, da zu den m Dimensionen von x noch die eine Dimension von y dazukommt.

Durch die Features ist es dann möglich den Datensätzen einen bestimmten Ort im n-dimensionalen Koordinatensystem zuzuordnen. Diese können dann auf Grund ihrer räumlichen Anordnung klassifiziert werden. Dabei soll der Abstand von beiden Gruppen von Punkten zu dem Trennobjekt maximal sein. Dadurch kann die Klassifizierung optimal durchgeführt werden. Andernfalls würden schon geringe Abweichungen von den Test-Datensätzen reichen, das ein Datensatz falsch klassifiziert würde. (\ref{SVM2}) 


\begin{dsafigure}
\begin{center}
	\includegraphics[width=0.5\textwidth]{\media svm.jpg}
	\caption{Da der Abstand zwischen den unterschiedlich klassifizierten Datensätzen maximiert werden soll, gilt die rote und nicht die blaue Linie als Trennelement.}
	\label{SVM1}
	\end{center}
\end{dsafigure}


Um die Trennlinie zu optimieren, gibt es auch eine mathematische Beschreibung. Da sich Ebenen auch in der Form $\langle x, w \rangle = b $ beschreiben lassen, folgt, dass wir die Datensätze auch etwas anders betrachten können (\ref{SVM2}).
Das daraus folgende mathematische Programm sieht wie folgt aus: 

\begin{align*}
	\operatorname{maximiere}_{w,b} \quad & \frac{2}{\norm{(w)}} \\
	\operatorname{sodass} \quad &y_{i}(\langle w,x_{i} \rangle -b) \geq 1 \quad \forall  i = 1,...,n ; \\
	&x \in \mathbb{R}^n; \quad	y \in \{1;-1\}
	 %\label{SVMProblemMax}
\end{align*}

Unter der Vorraussetzung, dass alle vorhandenen Datensätze richtig klassifiziert sind. Dieses Problem kann 
man aber genauso schreiben als:

\begin{align*}
		\operatorname{minimiere}_{w,b} \quad & \norm{(w)} \\
		\operatorname{sodass} \quad &y_{i}(\langle w,x_{i} \rangle -b) \geq 1 \quad \forall i = 1,...,n ; \\
		&x \in \mathbb{R}^n; \quad y \in \{1; -1\}
		%\label{SVMProblemMin}
\end{align*}
%Genauso gemacht wie obendrüber, funktioniert aber nicht geaunso ??

Klassifikation:

\begin{align*}
	y_{i} =& \{ 1 \operatorname{falls} \langle w,x_{i} \rangle - b \geq 1\} \\
		& \{ -1 \operatorname{falls} \langle w,x_{i} \rangle \leq 1\}
	%\label{KlassifikationSVM}
\end{align*}
%????

Nachdem das Problem optimiert wurde, ist der Computer in der Lage, weitere Daten einzuordnen, vorrausgesetzt, es ist richtig optimiert.
Dennoch kann es bei dieser Art von SVMs zu einigen Problemen kommen.\\

\begin{itemize}
	\item Wenn die Datensätze nicht linear separierbar sind, d.h., es ist nicht möglich die beiden Datensätze mit einem linearen Element zu trennen\\
	
	\item Wenn die Daten nicht alle korrekt klassifiziert sind oder es Daten gibt, die in dem >>Bereich<< der anderen Seite liegen.
\end{itemize}

\subsubsection{Soft Margin SVMs}
Die Soft Margin SVMs können mit dem zweiten Problem umgehen.
Sie sind im Prinzip wie herkömmliche SVMs aufgebaut, nur dass sie eine gewisse Fehlertoleranz besitzen. Diese kommt durch eine Veränderung an der Formel zustande:

\begin{align*}
	\operatorname{minimiere}_{w,b} \quad & \norm{w} + C\sum_{i}{z_{i}} \quad \forall i = 1,...,n \\
	\operatorname{sodass} \quad & y_{i}(\langle w,x_{i} \rangle - b) \geq 1 \quad \forall i = 1,...,n; \\ 
	&x \in \mathbb{R}^n; y \in \{1; -1\} \quad z_{i} \geq 0 \\
	%\label{eq: SoftMarginSVMs}
\end{align*}

Dabei steht $z_{i}$ für die Grösse des Fehlers des Punktes $x_{i}$ und C ist eine Konstante, deren grösse bestimmt, wie stark die Fehler gewichtet werden, da die Konstante mit der Summe der Fehler multipliziert wird. Da die gesamte Gleichung aber minimiert werden soll sorgt ein hohes C dafür, dass das Problem schwerer optimiert werden kann. Die Konstante muss vom Mensch selbst gewählt werden, je nachdem, wie schwer Fehler gewichtet werden sollen.

\begin{dsafigure}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{\media Figure_SVM.pdf}
		\caption{Links und rechts zur Trenngeraden befinden sich die parallelen Grenzen (gestrichelte Geraden). Ziel der Optimierung ist es, den Abstand zwischen den Grenzen zu maximieren, um den Normalenvektor $w$ zu bestimmen.}
		\label{FigSVM}
	\end{center}
\end{dsafigure}

\section{Neuronale Netze}
\author {Farhadiba Mohammed}

\subsection{Einführung}

Inspiriert von unserem Verständnis, wie das menschliche Gehirn lernt, benutzen Neuronale Netze Lernalgorithmen, welche besonders für praktische Anwendungen geeignet sind.
Dazu zählen Spracherkennung, Objekterkennung in Bildern und die Fähigkeit individuell passende Produkte vorzuschlagen, die dem Kunden gefallen könnten. 
Ein Neuronales Netz wird von mehreren Schichten aufgebaut. Ausgangsschicht ist dabei, die Datenschicht, auf die ein oder mehrere hidden layer folgen. Als Ausgabewert erhält man schließlich einen Vektor, welcher die Wahrscheinlichkeitsverteilung darstellt. Mit anderen Worten, wie wahrscheinlich es ist, dass das Ausgangsobjekt zu einer bestimmten Klasse gehört.

\begin{dsafigure}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{\media Figure_NN.pdf}
		\caption{Ein vollständig verbundenes neuronales Netzwerk mit $i$ Eingängen und $k$ Ausgängen, bestehend aus $n$ Schichten mit jeweils $m$ >>Neuronen<<.}
		\label{FigNN}
	\end{center}
\end{dsafigure}

\subsection{Deep Learning}

Mit Deep Learning beschreibt man die Neuronalen Netze, die über mehr als einen >>versteckte Schicht<< verfügen. Damit ist gemeint, dass sich zwischen Ein- und Ausgabeschicht weitere Schichten befinden. 

Um die Anzahl an Parametern zu vergrößern, werden diese auch über nicht-lineare Funktionen miteinander verknüpft. Dadurch kann eine höhere Genauigkeit erzielt werden. Jedoch kann eine zu hohe Anzahl an Parametern auch dazu führen, dass das Netzwerk >>overfitted<<, d.h., zu sehr an den Trainingsdatensatz angepasst, wird. Dann kann das Netzwerk neue, fremde Daten nicht mehr korrekt klassifizieren. 

Wie wir in der Abbildung 2.3 sehen können ist ein typisches Fully connected Neuronales Netz abgebildet. Als Basis findet man unten die Datenschicht mit ihren Eingabewerten in Form eines Vektors $x_1 ... x_i$. Diese Werte werden nun mit einem jeweiligen Faktor $w$ multipliziert. Hierbei handelt es sich um ein Skalarprodukt. Das Ergebnis wird nun im ersten hidden layer abgespeichert. Darauf wird eine nicht lineare Funktion angewendet und das Ganze wird erneut mit einem Faktor $w$ multipliziert. Dieser Vorgang wiederholt sich so lange bis uns schließlich ein Vektor mit seinen Komponenten $p_1 .. p_1$ zurückgegeben wird, welcher als Wahrscheinlichkeitsverteilung interpretiert wird. Anhand eines konkreten Beispiels würde es folgendes bedeuten:
Nehmen wir an wir haben ein Bild und wollen ermitteln zu welcher Klasse Haus, Tisch oder Stuhl das daurauf abgebildete Objekt gehört. Unsere inputs wären demnach die Pixel des Bildes. Diese durchlaufen nun das Neuronale Netz und wir erhalten eine Wahrscheinlichkeitsverteilung als Rückgabewert, welche uns mitteilt, dass es am wahrscheinlichsten ist, dass das abgebildete Objekt ein Objekt der Klasse Stuhl ist. Demnach ist die Klassifizierung vollzogen.


\begin{align*}
a_j^{(k)} = f^{(k)} (\langle w_{ji}^{(k)}, a_i^{(k-1)}\rangle) = f^{(k)} (\sum_{i=1}^{m^{(k-1)}} w_{ji}^{(k)},a_i^{(k-1)} )
\end{align*}

\subsection{Convolutional Neural Networks}

\begin{dsafigure}
	\begin{center}
		\includegraphics[width=0.5\textwidth]{\media Figure_ConvNN}
		\caption{Ein Convolutional Neural Network (CNN) (dt.: >>faltendes neurales Netzwerk<<) mit vier Eingängen ($x_1, ..., x_4$) und zwei Ausgängen ($p_1, p_2$). Dazwischen befindet sich eine Schicht aus drei >>Neuronen<<, die als Filter wirkt.}
		\label{FigConvNN}
	\end{center}
\end{dsafigure}


Bei den Convolutional Networks handelt es sich um ein Neuronales Netz mit einer vereinfachten Struktur. Diese kommt dadurch zu Stande, dass nun kein Fully Connected Neuronales Netzwerk mehr vorliegt. Das bedeutet nicht jeder der Knoten ist mit jedem verbunden, sondern nur jeweils die lokalen drei.


\subsection{SVMs als Neuronale Netze}

Wir haben bereits SVMs kennengelernt. Im folgenden wird verdeutlicht, wie man diese auch als Neuronales Netz ausdrücken kann. Charakteristisch für dieses Neuronale Netz ist, dass es nur einen Layer besitzt:

\begin{align*}
\min\limits_{b,w,z} \norm{w} + C  \sum_{i=1}^{n} z
\end {align*}

sodass:

\begin{align*}
&y_i (<w,x> - b) \geq 1 - z_i ; z_i \geq 0 \\
\implies &z_i \geq 1- y_i (\langle w,x \rangle - b) \text{ und } z_i \geq 0
\end {align*}


\begin{align*}
\min &\norm{w} + C \cdot \sum_{i=1}^{n} \max (0, 1-y_i (\langle w,x_i\rangle-b) \\
 = &\norm{w} + C \sum_{i=1}^{n} \ell ( 1-y_i (\langle w,x_i\rangle)-b) \\
=&\norm{w} + C \sum_{i=1}^{n} \ell (a_i)
\end{align*}


\subsection{Lineare Regression}

Die lineare Regression kann auch in Form von Neuronalen Netzen ausgedrückt werden. 


\begin{align*}
\min\limits_{w,b} 0,5 \sum_{i=1}^{n} ((\langle w,x_i\rangle- b) -y_i)^2
a_i = \langle w,x_i\rangle -b-y_i
\end{align*}


