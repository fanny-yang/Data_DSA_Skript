\subsection{Gradientenmethode}

Zur Berechnung von Minima gibt es nun verschiedene Möglichkeiten:
Entweder man verwendet die gewöhnliche Tiefpunktberechnung indem man den Wert $\hat{a}$ 
ermittelt, f\"ur welchen $h'(a)=0$ und $h''(a)>0$. 
Eine weitere Methode w\"are die sogenannte \emph{Gradientenabstiegsmethode}. Das Ziel der Gradientenmethode ist es, das Optimum einer Funktion zu finden. Ein anfänglicher Schätzwert wird dafür iterativ in Richtung des negativen Gradienten 
ver\"andert, womit man sich dem Minimum der Funktion möglichst schnell n\"ahert.

Die Iteration, die dabei ausgef\"uhrt wird, lautet folgenderma\ss en. 
\begin{equation*}
a^{t+1}  =a^{t} - \lambda \cdot \nabla  f(a^{t}).
\end{equation*}
Nach Lemma~\ref{lem:gradientabstieg} ist es moeglich, dass man ein $\lambda > 0$ findet, sodass $f(a^{t+1}) < f(a^{t})$ falls $ \nabla f(a^{(t)}) \neq 0$. 
Dies kann zum Beispiel wiederum durch ein Optimierungsproblem erreicht werden, und zwar $\lambda=\arg\min_{\lambda} g(\lambda)$ wobei $g(\lambda)=f(a^t-\lambda\nabla f(a^t))$.


%Folgend beschrieben wird Gradientenmethode, die einen wichtigen Teil des Gebietes der Optimierung darstellt.


\begin{dsafigure}
\begin{center}
\includegraphics[width=0.4\textwidth]{\media grafik_optimierung_patrice_gradient_descent.pdf}
\label{figure:grafik_optimierung_patrice_gradient_descent}
\caption{Beispiel der Gradientenmethode}
\end{center}
\end{dsafigure}

Eine mögliche Softwareimplementierung in Pseudocode wird hier gezeigt:

\begin{algorithmic}[1]

\Procedure{$\mathbf{calculateMinimum}$}{}
   \\$F(x) = (x^2-2)^2$
\\$F'(x) = 4x^3-8x$

\\
\\$x = 10.0$
\\$\lambda = 0.001$

\For{$i = 1, \dots, m$}
 \State $\lambda = \lambda+0.001\cdot i$
  \For{$j = 1, \dots, n$}
    \State $x = x - \lambda \cdot F'(x)$
  \EndFor
\EndFor
 
 
\\Print x\EndProcedure
\Statex
\end{algorithmic}

\paragraph{Erklärung}

$F(x)$ beschreibt die Funktion, deren globales Minimum wir finden wollen. $F'(x)$ ist dementsprechend die Ableitung der Funktion $F(x)$. Mit $x=10.0$ setzen wir den Schätzwert, ab dem optimiert wird. $\lambda$ bekommt einen niedrigen Wert deklariert, damit man sich in kleinen Schritten dem Minimum annähern kann. Dies wiederum geschieht in zwei For-Schleifen, deren Inhalt in diesem Fall je 8 mal durchlaufen wird. In der äußeren Schleife sorgen wir dafür, dass unser Lambda größere Werte annimmt. In der inneren For-Schleife wird die Schätz-Variable $x$ mit Hilfe der Ableitung und $\lambda$ angepasst. Daraus folgt
%\begin{equation*}
 $f(x-\lambda \nabla f(x)) \leq f(x)$.
% \end{equation*}

